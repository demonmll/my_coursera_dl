{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 train/dev/test sets（训练 验证 测试数据集）\n",
    "> 测试和验证数据集最好来自同一分布（特征工程）会使计算更快\n",
    "\n",
    "> 可以不使用测试数据集，但是一定需要有验证数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Bias Variance(偏差 方差)\n",
    "> 欠拟合-高偏差 过拟合-高方差\n",
    "\n",
    "> 图三->高偏差和高方差的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-2.png'>\n",
    "<img src='./picture/1-3.png'>\n",
    "<img src='./picture/1-4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Basic \"recipe\" for machine learning 机器学习基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regularization (正则化)\n",
    "> 通常w参数非常复杂，加入惩罚项降低w参数的复杂度\n",
    "\n",
    "> 注意逻辑回归和神经网络的正则化有些区别，神经网络的l2正则并不是矩阵相乘，而是矩阵中的各个元素平方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-6.png'>\n",
    "<img src='./picture/1-7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 why regularization reduces overfitting\n",
    "> lamda big -> w small -> z small -> module linear -> bais big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 dropout regularzation 随机失活正则化\n",
    "> **每一次迭代进行一次dropout,数据还是训练数据集**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-9.png'>\n",
    "<img src='./picture/1-10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 understanding dropout\n",
    "> dropout并不依赖某一项特征，收缩权重,减小过拟合带来的影响\n",
    "\n",
    "> 神经元较少的层keepprob=1.0（不使用dropout）神经元较多的层可以使用dropout\n",
    "\n",
    "> **使用dropout没有一个明确的lost函数，需要当lost函数单调递减时再使用dropout会有很好的效果**\n",
    "\n",
    "<img src='./picture/1-11.png'>\n",
    "<img src='./picture/1-12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 other regularization methods（其他几种正则化的方法）\n",
    "> 除了l2正则化和dropout正则化，还有几种方法可以减少神经网络中的过拟合\n",
    "\n",
    "> 1) 数据扩增（Data augmentation）：通过图片的一些变换，得到更多的训练集和验证集\n",
    "\n",
    "<img src='./picture/1-13.png'>\n",
    "> 2) Early stopping：在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决bias和variance之间的最优。 \n",
    "\n",
    "<img src='./picture/1-14.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing inputs (归一化)\n",
    ">当数据规模不一致时 -1-1 1-1000000 优化速度会非常低\n",
    "\n",
    "> 归一化以较少的次数达到全局最优\n",
    "\n",
    "<img src='./picture/1-15.png'>\n",
    "<img src='./picture/1-16.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 vanishing/exploding gradients 梯度消失梯度爆炸\n",
    "> 情况对于导数也是同样的道理，所以在计算梯度时，根据情况的不同，梯度函数会以指数级递增或者递减，导致训练导数难度上升，梯度下降算法的步长会变得非常非常小，需要训练的时间将会非常长。\n",
    "\n",
    "<img src='./picture/1-17.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 weight initialization for deep networks （权重初始化）\n",
    "> **对权重参数初始化乘以一个参数的原因：将输出的z值尽可能的降低,降低梯度爆炸的速度**\n",
    "\n",
    "<img src='./picture/1-18.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 numerical approximation of gradients(梯度的数值逼近)\n",
    "> 用双边误差有更高的精度\n",
    "\n",
    "<img src='./picture/1-19.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 Gradient Checking (梯度检验)\n",
    "> 偏导的定义：保持其他的参数不变，只改变一个参数+ e\\ -e计算结果，用欧式距离计算比率\n",
    "\n",
    "> 首先-连接参数\n",
    "\n",
    "> 其次-计算梯度 只改变一个参数\n",
    "\n",
    "<img src='./picture/1-20.png'>\n",
    "<img src='./picture/1-21.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Gradient Checking iplementation notes(关于梯度检验实现的注记)\n",
    "> **在训练中使用梯度检验时间会相当长**\n",
    "\n",
    "> 不要忘记正则化对于参数的影响，不要和dropout一起使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-22.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
