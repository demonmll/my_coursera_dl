{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Mini-batch gradient descent（batch vs mini batch）\n",
    "> 每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。\n",
    "\n",
    "> 注意表示方法 大括号子集\n",
    "\n",
    "> mini-batch的梯度下降唯一的区别就是把原先的x换成了x{t} \n",
    "\n",
    "> \"1 epoch\"表示遍历数据集 ，batch方法1 epoch只能完成一次迭代 ，而mini-batch方法可以完成m次迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1.png'>\n",
    "<img src='./picture/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 understanding mini-batch gradient descent（理解 mini batch）\n",
    "> 普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势\n",
    "\n",
    "> 如果训练样本的大小比较小时，如m⩽2000时 —— 选择batch梯度下降法\n",
    "\n",
    "> 如果训练样本的大小比较大时，mini-batch典型的大小为： 2^6、2^7、⋯、2^10 需要符合cpu/gpu内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/3.png'>\n",
    "<img src='./picture/4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 exponentaially weighted averages (指数加权移动平均值)\n",
    "> **Vt约等于 1/(1-b) 天的平均气温，比如当b=0.9时 ，1/（1-b）=10, vt就表示当前10天的平均值，只给了当前时间0.1的权重**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/5.png'>\n",
    "<img src='./picture/6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 understanding exponentially weighted averages (理解加权平均)\n",
    ">总体来说存在，(1−ε)^1/ε=1/e，1−ε=β=0.9，即0.9^10≈0.35≈1/e。相当于大约10天后，系数的峰值（这里是0.1）下降到原来的1/e**（权重减小）。**\n",
    "\n",
    ">在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，指数加权平均在节约计算成本的方面是一种非常有效的方式，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 bias corresction in exponentially weighted average(指数加权平均的偏差修正)\n",
    "> 由于初始值初始为0，所以最开始的一段与真实数值有偏差\n",
    "\n",
    "> 采用公式vt/(1-b^t)减小初始段的误差，虽然存在这种问题，但是在实际过程中，一般会忽略前期均值偏差的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/8.png'>\n",
    "<img src='./picture/9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Gradient descent with momentum (动量梯度下降)\n",
    "> 动量梯度下降的基本思想就是计算梯度的指数加权平均数，并利用该梯度来更新权重。\n",
    "\n",
    "> 结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率\n",
    "\n",
    "> 在纵轴方向梯度下降缓慢，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点,采用动量梯度下降\n",
    "\n",
    "> **在梯度下降中，每一次权重更新都是独立的，但在动量梯度下降中，每一次dw的值都采用指数加权平均值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/10.png'>\n",
    "<img src='./picture/11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 RMSprop\n",
    "> 类似共轭梯度法，在db和dw上除以了一个系数\n",
    "\n",
    "> 与动量梯度下降一样，可以使用更大的学习率来加快迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Adam optimization algorithm(adam 优化算法)\n",
    "> RMSprop和Monmetum是少有的两种可以用于大多数神经网络优化的算法，adam优化算法就是结合了这两种算法\n",
    "\n",
    "> 常用的机器学习算法，可以用于大量的神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/13.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Learning rate decay(学习率衰减)\n",
    "> 利用 mini-batch 梯度下降法不会精确收敛，而一直会在一个最小值点较大的范围内波动(学习率很大)\n",
    "\n",
    ">使用学习率衰减，在算法开始的时候，学习速率还是相对较快。但随着α的减小，最终会在最小值附近的一块更小的区域里波动.\n",
    "\n",
    "> epoch：为一次遍历，minibatch的1epoch迭代多次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/14.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 The problem of local optima(局部最优问题)\n",
    "> 低维度情况下可能会像左图出现局部最优的情况，**但是在神经网络高纬度的情况下，并不会出现这种情况，而是出现右图鞍点的情况。**在一个具有高维度空间的函数中，如果梯度为0，那么在每个方向，Cost function可能是凸函数，也有可能是凹函数。但如果参数维度为2万维，想要得到局部最优解，那么所有维度均需要是凹函数，其概率为2−20000，可能性非常的小。也就是说，在低纬度中的局部最优点的情况，并不适用于高纬度，我们在梯度为0的点更有可能是鞍点。\n",
    "\n",
    "\n",
    "\n",
    "> 在高纬度的情况下： \n",
    "* 几乎不可能陷入局部最小值点； \n",
    "* 处于鞍点的停滞区会减缓学习过程，利用如Adam等算法进行改善。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/15.png'>\n",
    "<img src='./picture/16.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
