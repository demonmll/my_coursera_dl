{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 neural networks overview\n",
    "> **方括号表示层 小括号表示样本**\n",
    "\n",
    "> **repeating logistiction twice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-1.png'>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 neural network representation\n",
    "> **using a[0] representation the input data(x)、a also means activation **\n",
    "\n",
    "> some symbol a[0] a[1] a[1]-1\n",
    "\n",
    "> the layer mounts dont count the input layer\n",
    "\n",
    "> **hidden layer also come with paramaters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-2.png'>\n",
    "\n",
    "<img src='./picture/1-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 computing a neural networks output\n",
    "> 上标表示层 下标表示节点\n",
    "\n",
    "> **注意节点的表示方法**\n",
    "\n",
    "> notice the matrix shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-4.png'>\n",
    "<img src='./picture/1-5.png'>\n",
    "<img src='./picture/1-6.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 vectorizing across multiple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-7.png'>\n",
    "<img src='./picture/1-8.png'>\n",
    "<img src='./picture/1-9.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Activation functions\n",
    "> some optitions better than sigmoid function\n",
    "\n",
    "> tanh function always work better than sigmoid function(only use sigmoid function in output layer)\n",
    "\n",
    "> **use relu function when z is lager , it makes d can descent quickly -> most popular**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-10.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 why need non-linear activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use non-linear activation functions can solve non-linear problems\n",
    "\n",
    "use linear activation neunal network is equal to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 derivatives of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-11.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 gradient descent for neural networks\n",
    "> **注意在计算对w2的导数时 a[1]充当x的角色，需要转置一次**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-13.png'>\n",
    "<img src='./picture/1-15.png'>\n",
    "<img src='./picture/1-12.png'>\n",
    "\n",
    "<img src='./picture/1-14.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Random Initialization\n",
    "> **usually initialize the weight a very small number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./picture/1-17.png'>\n",
    "<img src='./picture/1-18.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
